"""
Chat Service V3 - Arquitetura Metrics-First.

Arquitetura Metrics-First - Fase 4
Servi√ßo principal que orquestra o fluxo metrics-first.

Fluxo:
1. Query Interpreter (heur√≠stica-first)
2. Metrics Calculator (DuckDB otimizado)
3. Metrics Validator (Truth Contract) ‚ö†Ô∏è OBRIGAT√ìRIO
4. Context Builder (Markdown estruturado)
5. Narrative Generator (LLM controlada)
6. Chart Generator (opcional)

Princ√≠pios:
- Fluxo linear (sem loop)
- LLM nunca calcula
- Backend √© a fonte da verdade
- Valida√ß√£o obrigat√≥ria antes da LLM
"""

import logging
import asyncio
from typing import Dict, Any, Optional, Callable, Awaitable
from dataclasses import dataclass

# Componentes da arquitetura metrics-first
from app.services.query_interpreter import QueryInterpreter, NeedsClarificationError
from app.services.metrics_calculator import MetricsCalculator
from app.services.metrics_validator import validate_metrics, NoDataError, InvalidMetricError
from app.services.context_builder import ContextBuilder

# Componentes existentes
from app.core.llm_factory import LLMFactory
from app.core.utils.session_manager import SessionManager

# NOVO: Modelos Pydantic para Structured Output e Validation Guardrails
from app.core.models.llm_response import RespostaBI, validate_response_guardrails

logger = logging.getLogger(__name__)


@dataclass
class SystemResponse:
    """
    Resposta do sistema (n√£o gerada pela LLM).
    
    Usado para:
    - Erros (NoDataError, InvalidMetricError)
    - Esclarecimentos (NeedsClarificationError)
    - Mensagens do sistema
    """
    message: str
    type: str  # "no_data", "error", "clarification_needed", "system"
    suggestion: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicion√°rio para resposta API"""
        result = {
            "type": "text",
            "result": {
                "mensagem": self.message
            },
            "system_response": True,
            "response_type": self.type
        }
        
        if self.suggestion:
            result["result"]["sugestao"] = self.suggestion
        
        return result


class ChatServiceV3:
    """
    Servi√ßo de chat com arquitetura Metrics-First.
    
    Diferen√ßas do V2:
    - Sem LangGraph (fluxo linear)
    - Sem tool selection (heur√≠stica-first)
    - Valida√ß√£o obrigat√≥ria (Truth Contract)
    - LLM apenas para narrativa
    """
    
    def __init__(
        self,
        session_manager: SessionManager,
        parquet_path: Optional[str] = None
    ):
        """
        Args:
            session_manager: Gerenciador de sess√µes
            parquet_path: Caminho para o parquet (opcional)
        """
        self.session_manager = session_manager
        
        # Inicializar componentes
        logger.info("Inicializando ChatServiceV3 (Metrics-First)...")
        
        # LLM para query interpretation e narrative generation
        self.llm = LLMFactory.get_adapter(use_smart=True)
        
        # Componentes metrics-first
        self.query_interpreter = QueryInterpreter(llm_adapter=self.llm)
        self.metrics_calculator = MetricsCalculator(parquet_path=parquet_path)
        self.context_builder = ContextBuilder()
        
        logger.info("‚úÖ ChatServiceV3 inicializado com sucesso")
    
    async def process_message(
        self,
        query: str,
        session_id: str,
        user_id: str,
        on_progress: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None
    ) -> Dict[str, Any]:
        """
        Processa uma mensagem usando o fluxo metrics-first.
        
        Args:
            query: Query do usu√°rio
            session_id: ID da sess√£o
            user_id: ID do usu√°rio
            on_progress: Callback para eventos de progresso
        
        Returns:
            Dicion√°rio com resposta (compat√≠vel com API existente)
        """
        logger.info(f"[V3] Processando query: '{query[:100]}...'")
        
        try:
            # Callback helper
            async def emit_progress(tool: str, status: str):
                if on_progress:
                    await on_progress({
                        "type": "tool_progress",
                        "tool": tool,
                        "status": status
                    })
            
            # 1. OBTER HIST√ìRICO (Necess√°rio antes da interpreta√ß√£o para Contexto Stateful)
            # Fetch history early explicitly for the interpreter
            chat_history = self.session_manager.get_history(session_id, user_id)

            # 2. INTERPRETAR (10-200ms - heur√≠stica-first)
            await emit_progress("Interpretando pergunta", "start")
            
            user_context = {"user_id": user_id, "session_id": session_id}
            intent = await asyncio.to_thread(
                self.query_interpreter.interpret,
                query,
                user_context,
                chat_history  # ‚úÖ FIX 2026-01-17: Pass History for Entity Carry-Over
            )
            
            logger.info(f"Intent: {intent.intent_type} (confian√ßa: {intent.confidence:.2f})")
            await emit_progress("Interpretando pergunta", "done")
            
            # 2. CALCULAR M√âTRICAS (50-300ms - DuckDB otimizado)
            await emit_progress("Consultando dados", "start")
            
            # Aplicar filtros do usu√°rio (RLS, etc)
            user_filters = self._get_user_filters(user_id)
            
            metrics = await asyncio.to_thread(
                self.metrics_calculator.calculate,
                intent.intent_type,
                intent.entities,
                intent.aggregations,
                user_filters
            )
            
            logger.info(f"M√©tricas calculadas: {metrics.row_count} linhas em {metrics.execution_time_ms:.0f}ms")
            await emit_progress("Consultando dados", "done")
            
            # 3. VALIDAR M√âTRICAS (1-5ms - Truth Contract) ‚ö†Ô∏è OBRIGAT√ìRIO
            await emit_progress("Validando dados", "start")
            validate_metrics(metrics)  # ‚Üê Levanta exce√ß√£o se inv√°lido
            await emit_progress("Validando dados", "done")
            
            # 4. CONSTRUIR CONTEXTO (10-50ms - Markdown estruturado)
            await emit_progress("Preparando contexto", "start")
            
            context = self.context_builder.build(metrics, intent)
            context_str = self.context_builder.to_string(context)
            
            logger.info(f"Contexto constru√≠do: ~{context.total_tokens} tokens")
            await emit_progress("Preparando contexto", "done")
            
            # 5. GERAR GR√ÅFICO (PRIORIT√ÅRIO)
            # Geramos o gr√°fico ANTES da narrativa para saber se ele existe e informar o LLM
            chart_data = None
            if intent.visualization:
                await emit_progress("Gerando gr√°fico", "start")
                chart_data = await self._generate_chart(metrics, intent)
                await emit_progress("Gerando gr√°fico", "done")

            # 6. GERAR NARRATIVA (500-1500ms - LLM controlada)
            await emit_progress("Gerando resposta", "start")
            
            # Obter hist√≥rico (J√° obtido no in√≠cio com chat_history, reutilizando)
            # chat_history = self.session_manager.get_history(session_id, user_id)
            
            response_text = await self._generate_narrative(
                context_str,
                chat_history,
                intent,
                has_chart=(chart_data is not None) # Informar se gr√°fico foi gerado
            )
            
            await emit_progress("Gerando resposta", "done")
            
            # Salvar no hist√≥rico
            self.session_manager.add_message(session_id, "user", query, user_id)
            self.session_manager.add_message(session_id, "assistant", response_text, user_id)
            
            # Montar resposta
            response = {
                "type": "text",
                "result": {
                    "mensagem": response_text
                }
            }
            
            if chart_data:
                response["chart_data"] = chart_data
            
            logger.info(f"[V3] Resposta gerada com sucesso")
            return response
        
        except NoDataError as e:
            # Resposta do sistema (n√£o da LLM)
            logger.warning(f"NoDataError: {e}")
            return SystemResponse(
                message=str(e),
                suggestion="Tente ampliar os crit√©rios de busca ou remover filtros espec√≠ficos",
                type="no_data"
            ).to_dict()
        
        except NeedsClarificationError as e:
            # Query amb√≠gua
            logger.warning(f"NeedsClarificationError: {e}")
            return SystemResponse(
                message=str(e),
                type="clarification_needed"
            ).to_dict()
        
        except InvalidMetricError as e:
            # M√©tricas inv√°lidas
            logger.error(f"InvalidMetricError: {e}", exc_info=True)
            return SystemResponse(
                message="Ocorreu um erro ao processar os dados. Tente novamente.",
                type="error"
            ).to_dict()
        
        except ValueError as e:
            # Erro de valida√ß√£o de par√¢metros
            logger.error(f"ValueError: {e}", exc_info=True)
            return SystemResponse(
                message=f"Par√¢metros inv√°lidos: {str(e)}",
                suggestion="Verifique os filtros e tente novamente",
                type="validation_error"
            ).to_dict()
        
        except TimeoutError as e:
            # Timeout na query
            logger.error(f"TimeoutError: {e}", exc_info=True)
            return SystemResponse(
                message="A consulta demorou muito para processar.",
                suggestion="Tente reduzir o per√≠odo ou adicionar mais filtros espec√≠ficos",
                type="timeout"
            ).to_dict()
        
        except Exception as e:
            # Erro gen√©rico
            logger.error(f"Erro no processamento: {e}", exc_info=True)
            return SystemResponse(
                message="Erro ao processar sua solicita√ß√£o. Por favor, tente novamente.",
                type="error"
            ).to_dict()
    
    async def _generate_narrative(
        self,
        context_str: str,
        chat_history: list,
        intent: Any,
        has_chart: bool = False
    ) -> str:
        """
        Gera narrativa usando LLM controlada.
        
        A LLM APENAS interpreta o contexto estruturado.
        N√£o calcula, n√£o consulta, n√£o decide.
        """
        # ‚úÖ Usar get_system_prompt() (sem par√¢metro version - consolidado)
        from app.core.prompts import get_system_prompt
        
        # Obter prompt com contexto sazonal se dispon√≠vel
        seasonal_context = None  # TODO: Detectar sazonalidade do contexto
        
        system_prompt = get_system_prompt(
            mode="default",
            has_chart=has_chart,
            seasonal_context=seasonal_context
        )
        
        logger.info(f"[MASTER_PROMPT] Usando prompt consolidado (has_chart={has_chart})")
        
        # Construir mensagens
        messages = []
        
        # Adicionar hist√≥rico (√∫ltimas 3 mensagens)
        for msg in chat_history[-3:]:
            role = "user" if msg["role"] == "user" else "assistant"
            messages.append({"role": role, "content": msg["content"]})
        
        # Adicionar contexto atual
        user_message = f"""CONTEXTO ESTRUTURADO (DADOS J√Å CALCULADOS E VALIDADOS):

{context_str}

---

INSTRU√á√ÉO:
Transforme os dados acima em uma resposta clara, profissional e acion√°vel.
Siga a estrutura de resposta definida no system prompt.
Destaque insights importantes e sugira a√ß√µes quando relevante.

RESPOSTA:"""
        
        messages.append({"role": "user", "content": user_message})
        
        # NOVO: Gerar resposta com Structured Output (Pydantic) e Validation Guardrails
        try:
            logger.info("[STRUCTURED OUTPUT] Gerando resposta estruturada...")
            
            # Solicitando JSON (Schema Adapt√°vel)
            if has_chart:
                # Schema Relaxado para Modo Visual (analise_detalhada opcional/curta)
                schema_instruction = """RETORNE JSON (MODO VISUAL - SEJA BREVE):
{{"filtros_mencionados":[],"resumo_executivo":"(Resumo em 1 frase)","analise_detalhada":"(Deixe vazio ou use max 1 frase)","insights":[],"recomendacoes":[],"dados_citados":true}}"""
            else:
                # Schema Padr√£o para Modo Anal√≠tico
                schema_instruction = """RETORNE JSON:
{{"filtros_mencionados":[],"resumo_executivo":"","analise_detalhada":"","insights":[],"recomendacoes":[],"dados_citados":true}}"""

            structured_msg = f"""{user_message}

{schema_instruction}
"""
            messages[-1] = {"role": "user", "content": structured_msg}
            
            response_text = await asyncio.to_thread(
                self.llm.generate_with_history,
                messages,
                system_instruction=system_prompt,
                max_tokens=600 if has_chart else 1000, # Menos tokens para modo visual
                temperature=0.3
            )
            
            # Parsear JSON
            import json, re
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                response_dict = json.loads(json_match.group(0))
                response_obj = RespostaBI(**response_dict)
                
                # VALIDATION GUARDRAILS
                validation = validate_response_guardrails(response_obj, intent, context_str)
                
                if validation.has_errors() and validation.corrected_response:
                    response_obj = validation.corrected_response
                    logger.info("[GUARDRAIL] Corrigido automaticamente")
                
                if validation.has_warnings():
                    logger.warning(f"[GUARDRAIL] Avisos: {validation.warnings}")
                
                # 1. Resumo Executivo -> Headline
                formatted = f"""## {response_obj.resumo_executivo}

{response_obj.analise_detalhada}

### üí° Insights Cruzados
{chr(10).join([f'- {i}' for i in response_obj.insights])}
"""
                if response_obj.recomendacoes:
                    formatted += f"""
### üöÄ Plano de A√ß√£o
{chr(10).join([f'- {r}' for r in response_obj.recomendacoes])}
"""
                logger.info(f"[STRUCTURED OUTPUT] Sucesso! Valid: {validation.is_valid}")
                return formatted.strip()
            else:
                raise ValueError("JSON n√£o encontrado")
        
        except Exception as e:
            logger.warning(f"[STRUCTURED OUTPUT] Erro: {e}. Fallback...")
            response = await asyncio.to_thread(
                self.llm.generate_with_history,
                messages,
                system_instruction=system_prompt,
                max_tokens=800
            )
            return response.strip()
    
    async def _generate_chart(self, metrics: Any, intent: Any) -> Optional[Dict]:
        """
        Gera especifica√ß√£o de gr√°fico.
        
        Usa a ferramenta existente de gera√ß√£o de gr√°ficos.
        """
        try:
            # Importar ferramenta de gr√°fico
            from app.core.tools.universal_chart_generator import gerar_grafico_universal_v2
            
            # Construir descri√ß√£o
            descricao = f"{intent.intent_type}"
            if "segmento" in intent.entities:
                descricao += f" {intent.entities['segmento']}"
            if "une" in intent.entities:
                descricao += f" loja {intent.entities['une']}"
            
            # Chamar ferramenta
            # FIX 2026-01-17: StructuredTool must be called via .invoke(args_dict)
            chart_args = {
                "descricao": descricao,
                "tipo_grafico": intent.visualization or "auto",
                "filtro_une": str(intent.entities.get("une", "")) if intent.entities.get("une") else None,
                "filtro_segmento": intent.entities.get("segmento", ""),
                "filtro_produto": str(intent.entities.get("produto", "")) if intent.entities.get("produto") else None
            }
            
            result = await asyncio.to_thread(
                gerar_grafico_universal_v2.invoke,
                chart_args
            )
            
            return result.get("chart_data")
        
        except Exception as e:
            logger.error(f"Erro ao gerar gr√°fico: {e}")
            return None
    
    def _get_user_filters(self, user_id: str) -> Dict[str, Any]:
        """
        Obt√©m filtros do usu√°rio (RLS, permiss√µes, etc).
        
        TODO: Implementar l√≥gica de RLS real
        """
        # Por enquanto, retornar vazio
        # No futuro: consultar permiss√µes do usu√°rio
        return {}
    
    def _should_use_reflection(self, narrative: str, intent: Any) -> bool:
        """
        Decide se deve usar Self-Reflection baseado na complexidade.
        
        Args:
            narrative: Narrativa gerada
            intent: Intent da query
        
        Returns:
            True se deve usar reflection, False caso contr√°rio
        """
        # Crit√©rio 1: Resposta longa (>500 chars)
        if len(narrative) > 500:
            return True
        
        # Crit√©rio 2: Intent complexo (compara√ß√£o, an√°lise detalhada)
        complex_intents = ["comparacao", "vendas", "ruptura"]
        if hasattr(intent, 'intent_type') and intent.intent_type.value in complex_intents:
            # Usar reflection em 30% dos casos complexos (evitar overhead)
            import random
            return random.random() < 0.3
        
        return False
    
    def close(self):
        """Fecha recursos"""
        if self.metrics_calculator:
            self.metrics_calculator.close()
        logger.info("ChatServiceV3 fechado")
